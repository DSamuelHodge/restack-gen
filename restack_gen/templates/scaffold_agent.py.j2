# @generated by restack-gen v{{ generator_version }} ({{ timestamp }})
# command: restack g scaffold {{ agent_name }}
"""{{ agent_name }} agent implementation (scaffold)."""

from datetime import timedelta
from enum import Enum
{% if with_llm %}from typing import Any
{% endif %}

from restack_ai.workflow import workflow, log
import os

from {{ project_name }}.common.compat import BaseModel
from {{ project_name }}.common.retries import DEFAULT_RETRY
from {{ project_name }}.common.settings import settings
{% if with_llm %}
from {{ project_name }}.common.llm_router import LLMRouter, LLMRequest
from {{ project_name }}.common.prompt_loader import PromptLoader
{% endif %}
{% if tools_server %}
from {{ project_name }}.common.fastmcp_manager import FastMCPClient
{% endif %}


class {{ event_enum_name }}(str, Enum):
	"""Events for {{ agent_name }} agent."""
	INIT = "init"  # Initial event


class {{ agent_name }}State(BaseModel):
	"""State model for {{ agent_name }} agent (extend as needed)."""
	status: str | None = None  # Initial status


@workflow.defn(name="{{ agent_name }}")
class {{ agent_name }}:
	"""
	Scaffolded agent with sensible defaults.

	- Long-lived workflow style agent with event handling.
{% if with_llm or tools_server %}
	- Enhanced with optional LLM and tool integrations.
{% endif %}
	"""
{% if with_llm or tools_server %}

	def __init__(self) -> None:
		"""Initialize agent with enhanced capabilities."""
{% if with_llm %}
		self.llm = LLMRouter()
		self.prompts = PromptLoader()
{% endif %}
{% if tools_server %}
		self.tools = FastMCPClient("{{ tools_server }}")
{% endif %}
{% endif %}

	@workflow.run
	async def run(self, initial_state: {{ agent_name }}State) -> None:
		"""
		Run {{ agent_name }} agent event loop.

		Args:
			initial_state: Initial agent state
		"""
		state = initial_state
		log.info("{{ agent_name }} started", extra={"initial_state": state})
{% if with_llm or tools_server %}

		# Example: Enhanced capabilities usage
{% if with_llm %}
		# Load versioned prompt
		prompt_template = await self.prompts.load("{{ agent_name|lower }}_prompt", version="1.0")
{% endif %}
{% if tools_server %}
		# Call tool to gather context
		tool_result = await self.tools.call_tool(
			"example_tool",
			{"query": "example"}
		)
{% endif %}
{% if with_llm %}
		# Format prompt with context
		prompt = prompt_template.format(
			state=state,
{% if tools_server %}
			tool_data=tool_result,
{% endif %}
		)
		# Call LLM
		_dry_run = str(os.getenv("RESTACK_LLM_DRY_RUN", "")).lower() in {"1", "true", "yes"}
		response = await self.llm.chat(LLMRequest(
			messages=[{"role": "user", "content": prompt}],
			temperature=0.7,
			dry_run=_dry_run,
		))
		log.info("LLM response", extra={"response": response.content, "usage": response.usage, "meta": response.metadata})
{% endif %}
{% endif %}

		while True:
			# Wait for next event or timeout
			event = await workflow.wait_condition(
				lambda: workflow.all_handlers_finished(),
				timeout=timedelta(seconds=settings.agent_event_timeout),
			)

			if event is None:
				log.info("{{ agent_name }} timeout - no events received")
				continue

			# TODO: Process events and update state

	@workflow.signal
	async def handle_event(self, event: {{ event_enum_name }}, payload: dict | None = None) -> None:
		"""Handle incoming events."""
		log.info(
			"{{ agent_name }} received event",
			extra={"event": event, "payload": payload},
		)

		# TODO: Implement event handling logic

	@workflow.query
	def get_state(self) -> {{ agent_name }}State:
		"""Query current agent state."""
		# TODO: Return current state
		raise NotImplementedError("State query not yet implemented")
